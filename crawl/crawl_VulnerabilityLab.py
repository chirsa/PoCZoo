import time
import random
from queue import Queue, Empty
from bs4 import BeautifulSoup as bs
from src.dataProceScript.spider_base import BaseSpider


class VulnerabilityLab(BaseSpider):
    def __init__(self, db, vulnName):
        super().__init__(db, vulnName)
        self.headers = {
            'Host': 'www.vulnerability-lab.com',
            'Cookie': 'PHPSESSID=p1j8fsbb59pjt4hr50a8vh34r0',
            # 放入自己的cookie
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.60',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.vulnerability-lab.com/get_content.php?id=2325',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Te': 'trailers',
            'Connection': 'keep-alive',
        }


    def get_url(self):
        url_list = []
        for i in range(1, 2328):
            yield f"https://www.vulnerability-lab.com/get_content.php?id={str(i)}"
        

    def spider(self, url):
        try:
            r = self.get(url, headers=self.headers)
            if r.status_code != 200:
                self.logger.warning(f"请求失败: {url}")
                return
            soup = bs(r.content, 'lxml')
            # 从 <body> 中获取 <pre> 标签
            body = soup.text
            time.sleep(random.randint(1, 3))
            # 将漏洞信息插入MongoDB数据库
            vulnerability = {
                "text": body,
            }
            # print(vulnerability)
            self.collection.insert_one(vulnerability)
        except Exception as e:
                self.logger.error(f"处理 {url} 时出错: {str(e)}")

    def run(self):
        try:
            # 清空历史数据
            self.collection.drop()
            self.logger.info(f"{self.vulnName} 清空历史数据")
             # 顺序处理所有URL
            success_count = 0
            for url in self.get_url():
                try:
                    self.spider(url)
                    success_count += 1
                except Exception as e:
                    self.logger.error(f"处理 {url} 时严重错误: {str(e)}")

                # 添加进度提示
                if success_count % 50 == 0:
                    self.logger.info(f"已处理 {success_count} 条数据")

            # 最终统计
            self.count = self.collection.count_documents({})
            # total = self.collection.count_documents({})
            self.logger.info(f'完成爬取！共计 {self.count} 条有效数据')
            
        except KeyboardInterrupt:
            self.logger.warning("用户中断爬取进程")
        finally:
            self.logger.info("爬虫运行结束")  




